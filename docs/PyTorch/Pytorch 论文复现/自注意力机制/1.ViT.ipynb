{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vision Transformer（ViT）是一种基于Transformer的视觉注意力模型，用于图像分类和其他计算机视觉任务。与传统卷积神经网络（CNN）不同，ViT使用自注意力机制来捕捉图像中的关键特征。该模型将输入图像分割成小块，并将每个块作为序列元素输入到Transformer编码器中。这种方法在训练时可以捕捉到图像中的局部和全局特征，从而提高了模型的性能。ViT已被证明在许多视觉任务上具有很高的准确性，并成为计算机视觉领域的热门研究方向之一。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/xuehangcang/DeepLearning/main/static/ViT-Transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize(224),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "training_data = datasets.CIFAR10(root='data', train=True,download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(root='data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.PatchEmbedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"将 2 维图像转化 1 维序列的嵌入向量\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 3,  # 输入图像的颜色通道数，默认值为3\n",
    "                 patch_size: int = 16,  # 将输入图像转换为卷积核的大小，默认值为16\n",
    "                 embedding_dim: int = 768):  # 将图像转换嵌入的维度，默认值为768。\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        # 用卷积核将图像转换为嵌入向量\n",
    "        self.patcher = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=embedding_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "            padding=0\n",
    "        )\n",
    "        # 将卷积后的图像转换为 1 维序列\n",
    "        self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        image_resolution = x.shape[-1]  # 输入图像的分辨率\n",
    "        assert image_resolution % self.patch_size == 0, f\"图像必须能够被卷积核整除{image_resolution},{self.patch_size}\"\n",
    "        x_patched = self.patcher(x)\n",
    "        x_flattened = self.flatten(x_patched)\n",
    "        return x_flattened.permute(0, 2, 1)  # 将嵌入调整为最终维度，从 [batch_size, P^2•C, N] 转换为 [batch_size, N, P^2•C]。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.MLPBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"包含一个归一化层的多层感知器块，简称 MLP 块\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int = 768,  # ViT-Base 表中的隐藏大小D\n",
    "                 mlp_size: int = 3072,  # ViT-Base 表中的 MLP 大小\n",
    "                 dropout: float = 0.1):  # ViT-Base 表中的 MLP 的丢弃率\n",
    "\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)  # 层归一化\n",
    "        self.mlp = nn.Sequential(  # 多层感知器\n",
    "            nn.Linear(in_features=embedding_dim, out_features=mlp_size),  # 线性变换\n",
    "            nn.GELU(),  # GELU 激活函数\n",
    "            nn.Dropout(p=dropout),  # 丢弃\n",
    "            nn.Linear(in_features=mlp_size, out_features=embedding_dim),  # 线性变换\n",
    "            nn.Dropout(p=dropout)  # 丢弃\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)  # 归一化\n",
    "        x = self.mlp(x)  # 多层感知器\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.MSABlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSABlock(nn.Module):\n",
    "    \"\"\"创建一个多头自注意力块，简称 MSA 块 \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int = 768,  # ViT-Base 表中的隐藏大小D\n",
    "                 num_heads: int = 12,  # ViT-Base 表中的头数\n",
    "                 attn_dropout: float = 0):  # ViT-Base 表中的注意力层的丢弃率\n",
    "\n",
    "        super(MSABlock, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)  # 层归一化\n",
    "        self.multihead_attn = nn.MultiheadAttention(  # 多头注意力\n",
    "            embed_dim=embedding_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=attn_dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output, _ = self.multihead_attn(query=x, key=x, value=x, need_weights=False)\n",
    "        return attn_output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"创建一个Transformer编码器块\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int = 768,  # ViT-Base 表中的隐藏大小D\n",
    "                 num_heads: int = 12,  # ViT-Base 表中的头数\n",
    "                 mlp_size: int = 3072,  # ViT-Base 表中的 MLP 大小\n",
    "                 mlp_dropout: float = 0.1,  # ViT-Base 表中的 MLP 的丢弃率\n",
    "                 attn_dropout: float = 0):  # ViT-Base 表中的注意力层的丢弃率\n",
    "        super(Encoder, self).__init__()\n",
    "        self.msa_block = MSABlock(  # 多头自注意力块\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_heads=num_heads,\n",
    "            attn_dropout=attn_dropout\n",
    "        )\n",
    "        self.mlp_block = MLPBlock(  # 多层感知器块\n",
    "            embedding_dim=embedding_dim,\n",
    "            mlp_size=mlp_size,\n",
    "            dropout=mlp_dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.msa_block(x) + x  # 多头自注意力块\n",
    "        x = self.mlp_block(x) + x  # 多层感知器块\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"默认情况下使用 ViT-Base 超参数创建一个 Vision Transformer 架构\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 img_size: int = 224,  # ViT论文中表3的训练分辨率, 输入图像的大小，默认值为224\n",
    "                 in_channels: int = 3,  # 输入图像通道数\n",
    "                 patch_size: int = 16,  # 将输入图像转换为卷积核的大小，默认值为16\n",
    "                 num_transformer_layers: int = 12,  # ViT-Base 表中的编码器层数\n",
    "                 embedding_dim: int = 768,  # ViT-Base 表中的隐藏大小D\n",
    "                 mlp_size: int = 3072,  # ViT-Base 表中的 MLP 大小\n",
    "                 num_heads: int = 12,  # ViT-Base 表中的头数\n",
    "                 attn_dropout: float = 0,  # ViT-Base 表中的注意力层的丢弃率\n",
    "                 mlp_dropout: float = 0.1,  # ViT-Base 表中的 MLP 的丢弃率\n",
    "                 embedding_dropout: float = 0.1,  # ViT-Base 表中的嵌入层的丢弃率\n",
    "                 num_classes: int = 1000):  # ViT-Base 表中的分类数\n",
    "\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        assert img_size % patch_size == 0, f\"图像必须能够被卷积核整除，{img_size},{patch_size}\"\n",
    "\n",
    "        self.num_patches = (img_size * img_size) // patch_size ** 2  # 计算图像中的卷积核数量\n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),  # 类别嵌入\n",
    "                                            requires_grad=True)\n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches + 1, embedding_dim),  # 位置嵌入\n",
    "                                               requires_grad=True)\n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)  # 嵌入层的丢弃率\n",
    "        self.patch_embedding = PatchEmbedding(in_channels=in_channels, patch_size=patch_size,  # 卷积嵌入\n",
    "                                              embedding_dim=embedding_dim)\n",
    "        self.transformer_encoder = nn.Sequential(  # 编码器\n",
    "            *[Encoder(\n",
    "                embedding_dim=embedding_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_size=mlp_size,\n",
    "                mlp_dropout=mlp_dropout) for _ in\n",
    "                range(num_transformer_layers)]\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(  # 分类器\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]  # 获取批次大小\n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1)  # 扩展类别嵌入\n",
    "        x = self.patch_embedding(x)  # 卷积嵌入\n",
    "        x = torch.cat((class_token, x), dim=1)  # 将类别嵌入和卷积嵌入连接\n",
    "        x = self.position_embedding + x  # 位置嵌入\n",
    "        x = self.embedding_dropout(x)  # 嵌入层的丢弃率\n",
    "        x = self.transformer_encoder(x)  # 编码器\n",
    "        x = self.classifier(x[:, 0])  # 分类器\n",
    "        return x\n",
    "model = VisionTransformer()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"训练\"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # 计算预测和损失\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    \"\"\"测试\"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"测试错误率: {(100 * (1 - correct)):.2f}%, 平均损失: {test_loss:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 7.044998 [    0/50000]\n",
      "loss: 2.129542 [ 6400/50000]\n",
      "loss: 2.100320 [12800/50000]\n",
      "loss: 2.206093 [19200/50000]\n",
      "loss: 2.299979 [25600/50000]\n",
      "loss: 2.141643 [32000/50000]\n",
      "loss: 2.110022 [38400/50000]\n",
      "loss: 2.037074 [44800/50000]\n",
      "测试错误率: 79.29%, 平均损失: 0.032642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 26 00:25:39 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.41                 Driver Version: 531.41       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090       WDDM | 00000000:01:00.0  On |                  N/A |\n",
      "| 77%   60C    P2              256W / 350W|  13845MiB / 24576MiB |     75%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      8076    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      8560    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A      9172    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     13496    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     13868    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13896    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14560    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     15880    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     17980    C+G   ...\\Docker\\frontend\\Docker Desktop.exe    N/A      |\n",
      "|    0   N/A  N/A     23168    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     26016    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     32816      C   C:\\Program Files\\Python39\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     33748    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "|    0   N/A  N/A     36044    C+G   ...\\PyCharm 2022.3.2\\bin\\pycharm64.exe    N/A      |\n",
      "|    0   N/A  N/A     39172    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     40028    C+G   ... VS Code\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
