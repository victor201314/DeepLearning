{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://github.com/xuehangcang/DeepLearning/blob/main/docs/PyTorch/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/5.构建神经网络.ipynb\" download=\"\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"110\" height=\"20\" role=\"img\" aria-label=\"jupyter: notebook\"><title>jupyter: notebook</title><linearGradient id=\"s\" x2=\"0\" y2=\"100%\"><stop offset=\"0\" stop-color=\"#bbb\" stop-opacity=\".1\"/><stop offset=\"1\" stop-opacity=\".1\"/></linearGradient><clipPath id=\"r\"><rect width=\"110\" height=\"20\" rx=\"3\" fill=\"#fff\"/></clipPath><g clip-path=\"url(#r)\"><rect width=\"49\" height=\"20\" fill=\"#555\"/><rect x=\"49\" width=\"61\" height=\"20\" fill=\"#fe7d37\"/><rect width=\"110\" height=\"20\" fill=\"url(#s)\"/></g><g fill=\"#fff\" text-anchor=\"middle\" font-family=\"Verdana,Geneva,DejaVu Sans,sans-serif\" text-rendering=\"geometricPrecision\" font-size=\"110\"><text aria-hidden=\"true\" x=\"255\" y=\"150\" fill=\"#010101\" fill-opacity=\".3\" transform=\"scale(.1)\" textLength=\"390\">jupyter</text><text x=\"255\" y=\"140\" transform=\"scale(.1)\" fill=\"#fff\" textLength=\"390\">jupyter</text><text aria-hidden=\"true\" x=\"785\" y=\"150\" fill=\"#010101\" fill-opacity=\".3\" transform=\"scale(.1)\" textLength=\"510\">notebook</text><text x=\"785\" y=\"140\" transform=\"scale(.1)\" fill=\"#fff\" textLength=\"510\">notebook</text></g></svg></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1 构建神经网络\n",
    "\n",
    "神经网络由执行数据操作的层、模块组成。\n",
    "\n",
    "`torch.nn` 命名空间提供了构建自己的神经网络所需的所有构建块。\n",
    "\n",
    "PyTorch 中的每个模块都是 `nn.Module` 的子类。\n",
    "\n",
    "神经网络本身就是一个模块，由其他模块（层）组成。这种嵌套结构使得构建和管理复杂的体系结构变得容易。\n",
    "\n",
    "在接下来的章节中，我们将构建一个神经网络，用于对 FashionMNIST 数据集中的图像进行分类。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T12:30:19.670546Z",
     "end_time": "2023-04-22T12:30:19.730304Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 获取训练设备\n",
    "\n",
    "我们希望能够在硬件加速器上，如GPU或MPS上训练我们的模型，如果可用的话。\n",
    "\n",
    "让我们来检查一下 `torch.cuda` 或 `torch.backends.mps` 是否可用，否则我们就使用CPU。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T12:30:19.678549Z",
     "end_time": "2023-04-22T12:30:19.799289Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.3 定义类\n",
    "\n",
    "我们通过子类化 `nn.Module` 来定义神经网络，并在 `__init__` 中初始化神经网络层。\n",
    "\n",
    "每个 `nn.Module` 子类都在 `forward` 方法中实现对输入数据的操作。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T12:30:19.696010Z",
     "end_time": "2023-04-22T12:30:19.811527Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们创建了一个 `NeuralNetwork` 实例，并将其移动到 `device` 中，并打印出其结构。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T12:30:19.710304Z",
     "end_time": "2023-04-22T12:30:19.821233Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "为了使用这个模型，我们需要将输入数据传递给它。这将执行模型的`前向传播`，同时进行一些`反向传播`。请勿直接调用 `model.forward()` !\n",
    "\n",
    "将输入数据传递给模型后，会返回一个二维张量。\n",
    "\n",
    "其中 dim=0 对应于每个类别的 10 个原始预测值的输出，而 dim=1 对应于每个输出的各个值。\n",
    "\n",
    "我们可以通过将其通过 `nn.Softmax` 模块的实例来得到预测概率。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T12:30:19.725309Z",
     "end_time": "2023-04-22T12:30:19.822234Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.4 模型层\n",
    "\n",
    "让我们来分解一下FashionMNIST模型中的层。\n",
    "\n",
    "为了说明这一点，我们将取一个大小为 28x28 的样本小批量，其中包含 3 张图像，并观察当我们将它们通过网络传递时会发生什么。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "print(input_image.size())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T12:30:19.741793Z",
     "end_time": "2023-04-22T12:30:19.823236Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.1 nn.Flatten\n",
    "\n",
    "我们初始化 `nn.Flatten` 层，\n",
    "\n",
    "将每个 2D 的 28 x 28 的图像转换为一个连续的包含 784 个像素值的数组。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T12:30:19.756215Z",
     "end_time": "2023-04-22T12:30:19.823236Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.2 nn.Linear\n",
    "\n",
    "`linear layer`是一个模块，它使用其存储的权重和偏置对输入进行线性变换。\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T12:30:19.771616Z",
     "end_time": "2023-04-22T12:30:19.823236Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.3 nn.ReLU\n",
    "\n",
    "非线性激活函数是创建模型输入和输出之间复杂映射的关键。它们在线性变换之后应用，引入了非线性，帮助神经网络学习各种现象。\n",
    "\n",
    "在这个模型中，我们在线性层之间使用 `nn.ReLU`，但是还有其他激活函数可以在您的模型中引入非线性。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.3549, -0.0539, -0.4223, -0.5090, -0.5241,  0.0671,  0.2640,  0.3148,\n",
      "          0.4059, -0.0043,  0.2942, -0.3136, -0.4854, -0.4096,  0.6128, -0.0403,\n",
      "          0.3008,  0.0272,  0.5043, -0.0982],\n",
      "        [ 0.0611,  0.1303, -0.3039, -0.1856, -0.5054,  0.2207,  0.2165,  0.0329,\n",
      "          0.2755, -0.1567,  0.2323, -0.2620, -0.5719, -0.1951,  0.4347, -0.3115,\n",
      "          0.0223,  0.2554,  0.4202, -0.3879],\n",
      "        [ 0.3159,  0.2250, -0.6701,  0.0093, -0.4254,  0.2786,  0.3091,  0.0265,\n",
      "          0.3994,  0.0292,  0.4304, -0.3413, -0.1928, -0.2474,  0.5133,  0.1104,\n",
      "         -0.0613,  0.2951,  0.5740, -0.3400]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.3549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0671, 0.2640, 0.3148, 0.4059,\n",
      "         0.0000, 0.2942, 0.0000, 0.0000, 0.0000, 0.6128, 0.0000, 0.3008, 0.0272,\n",
      "         0.5043, 0.0000],\n",
      "        [0.0611, 0.1303, 0.0000, 0.0000, 0.0000, 0.2207, 0.2165, 0.0329, 0.2755,\n",
      "         0.0000, 0.2323, 0.0000, 0.0000, 0.0000, 0.4347, 0.0000, 0.0223, 0.2554,\n",
      "         0.4202, 0.0000],\n",
      "        [0.3159, 0.2250, 0.0000, 0.0093, 0.0000, 0.2786, 0.3091, 0.0265, 0.3994,\n",
      "         0.0292, 0.4304, 0.0000, 0.0000, 0.0000, 0.5133, 0.1104, 0.0000, 0.2951,\n",
      "         0.5740, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T12:30:19.787730Z",
     "end_time": "2023-04-22T12:30:19.823236Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.4 nn.Sequential\n",
    "\n",
    "`nn.Sequential` 是一个有序的模块容器。数据会按照定义的顺序依次通过所有的模块。你可以使用序列容器来快速组合一个网络，比如 `seq_modules`。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0407, -0.0436,  0.0431, -0.2591,  0.2633,  0.0719, -0.2602,  0.0352,\n          0.2142,  0.2105],\n        [-0.1104, -0.0551,  0.1166, -0.2316,  0.2608,  0.1021, -0.2817, -0.0521,\n          0.3963,  0.2970],\n        [-0.0170,  0.0762, -0.0094, -0.3774,  0.3341,  0.0778, -0.2287, -0.0870,\n          0.3413,  0.3867]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T12:30:19.803107Z",
     "end_time": "2023-04-22T12:30:19.824239Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4.5 nn.Softmax\n",
    "\n",
    "神经网络的最后一层线性层返回 `logits`，这些值被传递到 `nn.Softmax` 模块中。这些 logits 被缩放到值为 [0, 1] 的概率表示，代表模型对每个类别的预测概率。`dim` 参数指定了哪个维度上的值必须总和为 1。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0924, 0.0921, 0.1005, 0.0743, 0.1253, 0.1034, 0.0742, 0.0997, 0.1192,\n         0.1188],\n        [0.0837, 0.0884, 0.1050, 0.0741, 0.1213, 0.1035, 0.0705, 0.0887, 0.1389,\n         0.1258],\n        [0.0910, 0.0999, 0.0917, 0.0634, 0.1292, 0.1000, 0.0736, 0.0848, 0.1302,\n         0.1362]], grad_fn=<SoftmaxBackward0>)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "pred_probab"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T12:30:19.819235Z",
     "end_time": "2023-04-22T12:30:19.921749Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.5 模型参数\n",
    "\n",
    "神经网络中的许多层都是*参数化*的，即具有相关的权重和偏置项，这些参数在训练过程中被优化。子类化 `nn.Module` 会自动跟踪模型对象中定义的所有字段，并使所有参数可以使用模型的 `parameters()` 或 `named_parameters()` 方法进行访问。\n",
    "\n",
    "在此示例中，我们迭代每个参数，并打印其大小和值的预览。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0209,  0.0339, -0.0216,  ...,  0.0168, -0.0327, -0.0261],\n",
      "        [ 0.0091, -0.0300,  0.0073,  ...,  0.0261, -0.0312,  0.0122]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0057,  0.0282], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0081, -0.0040, -0.0374,  ...,  0.0388,  0.0413,  0.0277],\n",
      "        [ 0.0179, -0.0427, -0.0428,  ..., -0.0084, -0.0159,  0.0429]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0408,  0.0259], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0247,  0.0435,  0.0034,  ..., -0.0109,  0.0325, -0.0065],\n",
      "        [-0.0146, -0.0176,  0.0147,  ...,  0.0441, -0.0148, -0.0104]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0005,  0.0319], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T12:30:19.833234Z",
     "end_time": "2023-04-22T12:30:19.922831Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
